import OpenAI from 'openai'

export interface StreamChunk {
    content: string
    done: boolean
}

export function generateId() {
    return Math.random().toString(36).substring(2, 9)
}

// Configuration
const API_KEY = import.meta.env.VITE_OPENAI_API_KEY
const BASE_URL = import.meta.env.VITE_OPENAI_BASE_URL || 'https://api.openai.com/v1'
const USE_MOCK = import.meta.env.VITE_USE_MOCK === 'true' || !API_KEY

const openai = new OpenAI({
    apiKey: API_KEY,
    baseURL: BASE_URL,
    dangerouslyAllowBrowser: true // Since this is a client-side demo
})

/**
 * Unified interface for chat responses
 */
export async function* getChatResponse(messages: { role: string, content: string }[], options?: { signal?: AbortSignal }): AsyncGenerator<StreamChunk> {
    if (USE_MOCK) {
        yield* mockStreamResponse(messages)
    } else {
        yield* openaiStreamResponse(messages, options)
    }
}

/**
 * Real OpenAI Streaming Response
 */
async function* openaiStreamResponse(messages: { role: string, content: string }[], options?: { signal?: AbortSignal }): AsyncGenerator<StreamChunk> {
    try {
        const stream = await openai.chat.completions.create({
            model: import.meta.env.VITE_OPENAI_MODEL || 'gpt-3.5-turbo',
            messages: messages as any,
            stream: true,
        }, { signal: options?.signal })

        for await (const chunk of stream) {
            const content = chunk.choices?.[0]?.delta?.content || ''
            if (content) {
                yield { content, done: false }
            }
        }
        yield { content: '', done: true }
    } catch (error) {
        console.error('OpenAI Stream Error:', error)
        throw error
    }
}

/**
 * Simulates a streaming LLM response
 */
export async function* mockStreamResponse(messages: { role: string, content: string }[]): AsyncGenerator<StreamChunk> {
    const prompt = messages[messages.length - 1]?.content || ''
    const delay = (ms: number) => new Promise(resolve => setTimeout(resolve, ms))
    
    await delay(500) // Initial latency

    const englishResponse = `Here is a comprehensive simulated response for: "${prompt}"

## Markdown Features Demo

### 1. Typography & Styles
We can support **bold text**, *italicized emphasis*, ~~strikethrough~~, and \`inline code snippets\`. 

> "Innovation distinguishes between a leader and a follower."
>
> â€” *Steve Jobs*

### 2. Structured Lists
Here is a nested list example:
- **Frontend Technologies**
  - Vue 3 / Nuxt
  - React / Next.js
  - Svelte
- **Backend Technologies**
  1. Node.js
  2. Python (FastAPI/Django)
  3. Go

**Task List:**
- [x] Implement Streaming
- [x] Support Markdown
- [ ] Optimize Performance
- [ ] Add Voice Input

### 3. Code Blocks
We need to ensure syntax highlighting works correctly for different languages.

**Python:**
\`\`\`python
def fibonacci(n):
    if n <= 1:
        return n
    return fibonacci(n-1) + fibonacci(n-2)

print([fibonacci(i) for i in range(10)])
\`\`\`

**TypeScript (Interface):**
\`\`\`typescript
interface ChatMessage {
  id: string;
  role: 'user' | 'assistant';
  content: string;
  timestamp: number;
}
\`\`\`

### 4. Tables
Data representation needs to be clear.

| Feature | Status | Priority |
| :--- | :---: | :---: |
| **Streaming** | âœ… Ready | High |
| **History** | ğŸš§ In Progress | Medium |
| **Plugins** | âŒ Pending | Low |

### 5. Mathematical Formulas (LaTeX)
Advanced chatbots simulate reasoning with math.

**Maxwell's Equations:**
$$
\\nabla \\times \\mathbf{E} = -\\frac{\\partial \\mathbf{B}}{\\partial t}
$$

**Quadratic Formula:**
The solution is $x = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}$.

### 6. Long Content Testing
Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. 

---
*Simulation complete. Generated at ${new Date().toLocaleTimeString()}.*`

    const chineseResponse = `è¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹é—®é¢˜ï¼š"${prompt}" çš„ä¸­æ–‡æ¨¡æ‹Ÿå›å¤ã€‚

## Markdown åŠŸèƒ½å®Œæ•´æ¼”ç¤º

### 1. æ’ç‰ˆä¸æ ·å¼ (Typography)
æˆ‘ä»¬æ”¯æŒ **åŠ ç²—æ–‡æœ¬**ã€*æ–œä½“å¼ºè°ƒ*ã€~~åˆ é™¤çº¿~~ ä»¥åŠ \`è¡Œå†…ä»£ç \` çš„æ··åˆæ’ç‰ˆã€‚

> "æµ·å†…å­˜çŸ¥å·±ï¼Œå¤©æ¶¯è‹¥æ¯”é‚»ã€‚"
>
> â€” *ç‹å‹ƒã€Šé€æœå°‘åºœä¹‹ä»»èœ€å·ã€‹*

### 2. åˆ—è¡¨ç³»ç»Ÿ (Lists)
å¤šçº§åˆ—è¡¨æ¼”ç¤ºï¼š
- **å‰ç«¯æŠ€æœ¯æ ˆ**
  - Vue 3 / Nuxt.js (æ¨è)
  - React / Next.js
  - SolidJS
- **åç«¯æŠ€æœ¯æ ˆ**
  1. Spring Boot (Java)
  2. Gin (Go)
  3. FastAPI (Python)

**å¾…åŠäº‹é¡¹åˆ—è¡¨:**
- [x] å®Œæˆæµå¼ä¼ è¾“æ¨¡å—
- [x] æ”¯æŒä¸­æ–‡ Markdown æ¸²æŸ“
- [ ] ä¼˜åŒ–é•¿æ–‡æœ¬æ¸²æŸ“æ€§èƒ½
- [ ] å¢åŠ å¤šæ¨¡æ€æ”¯æŒ

### 3. ä»£ç é«˜äº® (Code Blocks)
æµ‹è¯•ä¸åŒè¯­è¨€çš„ä»£ç é«˜äº®æ˜¾ç¤ºæ•ˆæœã€‚

**Go è¯­è¨€ç¤ºä¾‹:**
\`\`\`go
package main

import "fmt"

func main() {
    messages := []string{"ä½ å¥½", "ä¸–ç•Œ", "LLM"}
    for i, msg := range messages {
        fmt.Printf("ç´¢å¼•: %d, å†…å®¹: %s\\n", i, msg)
    }
}
\`\`\`

**Vue ç»„ä»¶ç¤ºä¾‹:**
\`\`\`vue
<script setup lang="ts">
import { ref } from 'vue'
const count = ref(0)
</script>

<template>
  <button @click="count++">ç‚¹å‡»æ¬¡æ•°: {{ count }}</button>
</template>
\`\`\`

### 4. è¡¨æ ¼æ•°æ® (Tables)
å±•ç¤ºå¤æ‚æ•°æ®çš„å¯¹é½ä¸å±•ç¤ºã€‚

| æ¨¡å—åç§° | å¼€å‘çŠ¶æ€ | è´Ÿè´£äºº | ä¼˜å…ˆçº§ |
| :--- | :---: | :---: | :---: |
| **æ ¸å¿ƒå¼•æ“** | âœ… å·²ä¸Šçº¿ | å¼ ä¸‰ | P0 |
| **ç”¨æˆ·ç³»ç»Ÿ** | ğŸš§ å¼€å‘ä¸­ | æå›› | P1 |
| **æ”¯ä»˜ç½‘å…³** | âŒ å¾…æ’æœŸ | ç‹äº” | P2 |

### 5. æ•°å­¦å…¬å¼ (LaTeX)
æµ‹è¯•æ•°å­¦å…¬å¼æ¸²æŸ“èƒ½åŠ›ã€‚

**ç”±äº** $e^{i\\pi} + 1 = 0$ï¼Œæˆ‘ä»¬çŸ¥é“è¿™æ˜¯æ•°å­¦ä¸­æœ€ä¼˜ç¾çš„å…¬å¼ã€‚

**é«˜æ–¯ç§¯åˆ†:**
$$
\\int_{-\\infty}^{\\infty} e^{-x^2} dx = \\sqrt{\\pi}
$$

### 6. é•¿æ–‡æœ¬ä¸å¤è¯—è¯
**ã€Šå°†è¿›é…’ã€‹ - æç™½**

å›ä¸è§é»„æ²³ä¹‹æ°´å¤©ä¸Šæ¥ï¼Œå¥”æµåˆ°æµ·ä¸å¤å›ã€‚
å›ä¸è§é«˜å ‚æ˜é•œæ‚²ç™½å‘ï¼Œæœå¦‚é’ä¸æš®æˆé›ªã€‚
äººç”Ÿå¾—æ„é¡»å°½æ¬¢ï¼Œè«ä½¿é‡‘æ¨½ç©ºå¯¹æœˆã€‚
å¤©ç”Ÿæˆ‘æå¿…æœ‰ç”¨ï¼Œåƒé‡‘æ•£å°½è¿˜å¤æ¥ã€‚

---
*æ¨¡æ‹Ÿç”Ÿæˆç»“æŸã€‚æ—¶é—´ï¼š${new Date().toLocaleTimeString('zh-CN')}*`

    // Randomly select one response
    const response = Math.random() > 0.5 ? englishResponse : chineseResponse
    
    const chunkSize = 5
    for (let i = 0; i < response.length; i += chunkSize) {
        await delay(10) // Typing effect
        yield {
            content: response.slice(i, i + chunkSize),
            done: false
        }
    }
    
    yield { content: '', done: true }
}

